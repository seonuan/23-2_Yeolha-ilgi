{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>박지원의 <<열하일기>> 중 <도강록>은 어떤 주제를 가지고 있을까요?</strong>\n",
    "### LDA를 사용한 토픽모델링으로 알아보겠습니다.\n",
    "###### KoNLPy 출처: 박은정, 조성준, “KoNLPy: 쉽고 간결한 한국어 정보처리 파이썬 패키지”, 제 26회 한글 및 한국어 정보처리 학술대회 논문집, 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Okt\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import logging\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"6_24_신미.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open(\"stopwords_add_dogang.txt\", 'r',encoding='cp949') as stopw:\n",
    "    list_file = stopw.readlines()\n",
    "stopwords = list_file[0].split(\",\")\n",
    "\n",
    "okt = Okt()\n",
    "nouns = okt.nouns(text)\n",
    "words = [n for n in nouns if len(n) > 1]\n",
    "removed_words=[]\n",
    "removed_word = [x for x in words if x not in stopwords]\n",
    "removed_words.append(removed_word)\n",
    "\n",
    "\n",
    "\n",
    "#def chunk_list(lst, n):\n",
    "    #return [lst[i:i+n] for i in range(0, len(lst), n)]\n",
    "\n",
    "#words50_list = chunk_list(removed_words, 50)\n",
    "#words50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"6_25_임신.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open(\"stopwords_add_dogang.txt\", 'r',encoding='cp949') as stopw:\n",
    "    list_file = stopw.readlines()\n",
    "stopwords = list_file[0].split(\",\")\n",
    "\n",
    "okt = Okt()\n",
    "nouns = okt.nouns(text)\n",
    "words = [n for n in nouns if len(n) > 1]\n",
    "removed_word = [x for x in words if x not in stopwords]\n",
    "removed_words.append(removed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"6_26_계유.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open(\"stopwords_add_dogang.txt\", 'r',encoding='cp949') as stopw:\n",
    "    list_file = stopw.readlines()\n",
    "stopwords = list_file[0].split(\",\")\n",
    "\n",
    "okt = Okt()\n",
    "nouns = okt.nouns(text)\n",
    "words = [n for n in nouns if len(n) > 1]\n",
    "removed_word = [x for x in words if x not in stopwords]\n",
    "removed_words.append(removed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"6_27_갑술.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open(\"stopwords_add_dogang.txt\", 'r',encoding='cp949') as stopw:\n",
    "    list_file = stopw.readlines()\n",
    "stopwords = list_file[0].split(\",\")\n",
    "\n",
    "okt = Okt()\n",
    "nouns = okt.nouns(text)\n",
    "words = [n for n in nouns if len(n) > 1]\n",
    "removed_word = [x for x in words if x not in stopwords]\n",
    "removed_words.append(removed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"6_28_을해.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open(\"stopwords_add_dogang.txt\", 'r',encoding='cp949') as stopw:\n",
    "    list_file = stopw.readlines()\n",
    "stopwords = list_file[0].split(\",\")\n",
    "\n",
    "okt = Okt()\n",
    "nouns = okt.nouns(text)\n",
    "words = [n for n in nouns if len(n) > 1]\n",
    "removed_word = [x for x in words if x not in stopwords]\n",
    "removed_words.append(removed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7_1_정축.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open(\"stopwords_add_dogang.txt\", 'r',encoding='cp949') as stopw:\n",
    "    list_file = stopw.readlines()\n",
    "stopwords = list_file[0].split(\",\")\n",
    "\n",
    "okt = Okt()\n",
    "nouns = okt.nouns(text)\n",
    "words = [n for n in nouns if len(n) > 1]\n",
    "removed_word = [x for x in words if x not in stopwords]\n",
    "removed_words.append(removed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7_2_무인.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open(\"stopwords_add_dogang.txt\", 'r',encoding='cp949') as stopw:\n",
    "    list_file = stopw.readlines()\n",
    "stopwords = list_file[0].split(\",\")\n",
    "\n",
    "okt = Okt()\n",
    "nouns = okt.nouns(text)\n",
    "words = [n for n in nouns if len(n) > 1]\n",
    "removed_word = [x for x in words if x not in stopwords]\n",
    "removed_words.append(removed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7_3_기묘.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open(\"stopwords_add_dogang.txt\", 'r',encoding='cp949') as stopw:\n",
    "    list_file = stopw.readlines()\n",
    "stopwords = list_file[0].split(\",\")\n",
    "\n",
    "okt = Okt()\n",
    "nouns = okt.nouns(text)\n",
    "words = [n for n in nouns if len(n) > 1]\n",
    "removed_word = [x for x in words if x not in stopwords]\n",
    "removed_words.append(removed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7_4_경진.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open(\"stopwords_add_dogang.txt\", 'r',encoding='cp949') as stopw:\n",
    "    list_file = stopw.readlines()\n",
    "stopwords = list_file[0].split(\",\")\n",
    "\n",
    "okt = Okt()\n",
    "nouns = okt.nouns(text)\n",
    "words = [n for n in nouns if len(n) > 1]\n",
    "removed_word = [x for x in words if x not in stopwords]\n",
    "removed_words.append(removed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7_5_신사.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open(\"stopwords_add_dogang.txt\", 'r',encoding='cp949') as stopw:\n",
    "    list_file = stopw.readlines()\n",
    "stopwords = list_file[0].split(\",\")\n",
    "\n",
    "okt = Okt()\n",
    "nouns = okt.nouns(text)\n",
    "words = [n for n in nouns if len(n) > 1]\n",
    "removed_word = [x for x in words if x not in stopwords]\n",
    "removed_words.append(removed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7_6_임오.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open(\"stopwords_add_dogang.txt\", 'r',encoding='cp949') as stopw:\n",
    "    list_file = stopw.readlines()\n",
    "stopwords = list_file[0].split(\",\")\n",
    "\n",
    "okt = Okt()\n",
    "nouns = okt.nouns(text)\n",
    "words = [n for n in nouns if len(n) > 1]\n",
    "removed_word = [x for x in words if x not in stopwords]\n",
    "removed_words.append(removed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7_7_계미.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open(\"stopwords_add_dogang.txt\", 'r',encoding='cp949') as stopw:\n",
    "    list_file = stopw.readlines()\n",
    "stopwords = list_file[0].split(\",\")\n",
    "\n",
    "okt = Okt()\n",
    "nouns = okt.nouns(text)\n",
    "words = [n for n in nouns if len(n) > 1]\n",
    "removed_word = [x for x in words if x not in stopwords]\n",
    "removed_words.append(removed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7_8_갑신.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open(\"stopwords_add_dogang.txt\", 'r',encoding='cp949') as stopw:\n",
    "    list_file = stopw.readlines()\n",
    "stopwords = list_file[0].split(\",\")\n",
    "\n",
    "okt = Okt()\n",
    "nouns = okt.nouns(text)\n",
    "words = [n for n in nouns if len(n) > 1]\n",
    "removed_word = [x for x in words if x not in stopwords]\n",
    "removed_words.append(removed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"7_9_을유.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open(\"stopwords_add_dogang.txt\", 'r',encoding='cp949') as stopw:\n",
    "    list_file = stopw.readlines()\n",
    "stopwords = list_file[0].split(\",\")\n",
    "\n",
    "okt = Okt()\n",
    "nouns = okt.nouns(text)\n",
    "words = [n for n in nouns if len(n) > 1]\n",
    "removed_word = [x for x in words if x not in stopwords]\n",
    "removed_words.append(removed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.487482475656726\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.callbacks import CoherenceMetric\n",
    "from gensim import corpora\n",
    "from gensim.models.callbacks import PerplexityMetric\n",
    "\n",
    "dictionary = corpora.Dictionary(removed_words)\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(words) for words in removed_words]\n",
    "\n",
    "num_topics = 5\n",
    "chunksize = 100\n",
    "passes = 20\n",
    "iterations = 40\n",
    "eval_every = None\n",
    "\n",
    "temp = dictionary[0]\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=5,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "print(model.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "\n",
    "def find_optimal_number_of_topics(dictionary, corpus, processed_data):\n",
    "    limit = 40;\n",
    "    start = 2;\n",
    "    step = 6;\n",
    "\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=processed_data, start=start, limit=limit, step=step)\n",
    "\n",
    "    x = range(start, limit, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    processed_data = [sent.strip().split(\",\") for sent in tqdm(open('./data/tokenized_data.csv', 'r', encoding='utf-8').readlines())]\n",
    "\n",
    "    # 정수 인코딩과 빈도수 생성\n",
    "    dictionary = corpora.Dictionary(processed_data)\n",
    "\n",
    "    # 출현빈도가 적거나 자주 등장하는 단어는 제거\n",
    "    dictionary.filter_extremes(no_below=10, no_above=0.05)\n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_data]\n",
    "    print('Number of unique tokens: %d' % len(dictionary))\n",
    "    print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    # 최적의 토픽 수 찾기\n",
    "    find_optimal_number_of_topics(dictionary, corpus, processed_data)\n",
    "출처: https://joyhong.tistory.com/138 [옳은 길로..:티스토리]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -11.2417.\n",
      "[([(0.028453507, '평양'),\n",
      "   (0.01250971, '봉황'),\n",
      "   (0.009006807, '중국'),\n",
      "   (0.007603942, '구경'),\n",
      "   (0.0076039415, '요양'),\n",
      "   (0.0076039415, '장복'),\n",
      "   (0.0076037226, '요동'),\n",
      "   (0.0065372735, '옛날'),\n",
      "   (0.0063774786, '나무'),\n",
      "   (0.0063774786, '어깨'),\n",
      "   (0.006377478, '군뢰'),\n",
      "   (0.0063772677, '패수'),\n",
      "   (0.0060273255, '벽돌'),\n",
      "   (0.0057816, '우리나라'),\n",
      "   (0.005628836, '시대'),\n",
      "   (0.0051510143, '역관'),\n",
      "   (0.0051510143, '정사'),\n",
      "   (0.005151014, '동쪽'),\n",
      "   (0.0051510134, '안시성'),\n",
      "   (0.0051510134, '사공')],\n",
      "  -8.306167905229698),\n",
      " ([(0.031078493, '벽돌'),\n",
      "   (0.013657371, '우리나라'),\n",
      "   (0.012966218, '가마'),\n",
      "   (0.011695135, '정사'),\n",
      "   (0.010423911, '책문'),\n",
      "   (0.01042391, '물건'),\n",
      "   (0.01042391, '나무'),\n",
      "   (0.00788146, '장사꾼'),\n",
      "   (0.00788146, '소나무'),\n",
      "   (0.007881337, '아침'),\n",
      "   (0.0066102347, '불길'),\n",
      "   (0.0066100345, '제도'),\n",
      "   (0.006607419, '석회'),\n",
      "   (0.0057522412, '빛깔'),\n",
      "   (0.0053390106, '기운'),\n",
      "   (0.0053390106, '압록강'),\n",
      "   (0.00533901, '자루'),\n",
      "   (0.00533901, '노음'),\n",
      "   (0.005339009, '연경'),\n",
      "   (0.005339009, '불꽃')],\n",
      "  -10.660954124183394),\n",
      " ([(0.02594293, '우리나라'),\n",
      "   (0.019464161, '벽돌'),\n",
      "   (0.017325228, '가마'),\n",
      "   (0.009838964, '나무'),\n",
      "   (0.008769498, '기와'),\n",
      "   (0.008769496, '돼지'),\n",
      "   (0.0077000298, '수레'),\n",
      "   (0.0077000298, '진흙'),\n",
      "   (0.006630563, '어깨'),\n",
      "   (0.0055610975, '요동'),\n",
      "   (0.005561097, '제도'),\n",
      "   (0.0055610966, '처음'),\n",
      "   (0.005561095, '역관'),\n",
      "   (0.005561095, '고을'),\n",
      "   (0.0045238812, '이야기'),\n",
      "   (0.004513311, '중국'),\n",
      "   (0.0044916286, '압록강'),\n",
      "   (0.0044916286, '허리'),\n",
      "   (0.004491628, '안장'),\n",
      "   (0.004491628, '호랑이')],\n",
      "  -10.866405956776815),\n",
      " ([(0.022858718, '청대'),\n",
      "   (0.020476863, '학자'),\n",
      "   (0.015722478, '책문'),\n",
      "   (0.013340266, '마음'),\n",
      "   (0.011002969, '벽돌'),\n",
      "   (0.00857584, '물건'),\n",
      "   (0.0078483205, '중국'),\n",
      "   (0.007384731, '오지'),\n",
      "   (0.0073847305, '얼굴'),\n",
      "   (0.0073736906, '문인'),\n",
      "   (0.0073543834, '우리나라'),\n",
      "   (0.00720423, '모양'),\n",
      "   (0.0069206855, '시대'),\n",
      "   (0.0061936257, '역관'),\n",
      "   (0.0061936257, '정사'),\n",
      "   (0.006193625, '호통'),\n",
      "   (0.0061936225, '명말'),\n",
      "   (0.0059722965, '높이'),\n",
      "   (0.0056269825, '천하'),\n",
      "   (0.0054084826, '연경')],\n",
      "  -11.071978731324043),\n",
      " ([(0.024683148, '군사'),\n",
      "   (0.015898824, '조선'),\n",
      "   (0.012539891, '명병'),\n",
      "   (0.012357567, '요양'),\n",
      "   (0.010665605, '요동'),\n",
      "   (0.009180956, '청병'),\n",
      "   (0.009078338, '함락'),\n",
      "   (0.008061314, '어른'),\n",
      "   (0.008061314, '패수'),\n",
      "   (0.008061313, '숭정'),\n",
      "   (0.0069416678, '압록강'),\n",
      "   (0.0069416673, '비장'),\n",
      "   (0.006940292, '사기'),\n",
      "   (0.006928182, '누루하치'),\n",
      "   (0.0069220355, '고구려'),\n",
      "   (0.005822024, '모양'),\n",
      "   (0.005822024, '평양'),\n",
      "   (0.005822022, '울음'),\n",
      "   (0.0056810053, '태종'),\n",
      "   (0.00517292, '심양')],\n",
      "  -15.302895305097604)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpype1140_py3109",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
